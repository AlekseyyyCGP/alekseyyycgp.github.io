---
layout: post
title: "Principal Component Analysis with NumPy"
date: 2020-06-21

description: "In where I implement a dimensionaly reduction technique called Principal Component Analysis (mostly) from scratch with NumPy"
certid: 2KMTVQFTY478
projectid: pca
---

_Principal Component Analysis_ is a dimensional reduction procedure in where "given a collection of two, three, or higher dimensional space, a 'best fitting' line can be defined as one that minimizes the average squared distance from a point to the line." (quoted from Wikipedia, [June 9, 2020 Revision](https://en.wikipedia.org/w/index.php?title=Principal_component_analysis&oldid=961671758)) In this project, I will implement the method of principal component analysis (PCA) mostly from scratch. I will use the NumPy library to do the computational linear algebra involved in PCA. I will also use pandas, matplotlib and seaborn to load the data, provide basic graphing functionality and extend matplotlib's basic plotting mechanism repectively.

__Task 1: Getting set up__

First, I will start by importing the libraries needed to do the analysis:

```python
%matplotlib inline
import pandas as pd # do a "pip3 install pandas" if you don't have it.
import numpy as np # do a "pip3 install numpy" if you don't have it.
import seaborn as sns # do a "pip3 install seaborn" if you don't have it.
import matplotlib.pyplot as plt # do a "pip3 install matplotlib" if you don't have it.
```

Next, I'll tweak some of the aesthetics configurations of the plot. I'll use a _R Lang "ggplot"_ styled plot and set the plot's dimensions (in inches) to $$12 \times 8$$:

```python
plt.style.use("ggplot")
plt.rcParams["figure.figsize"] = (12,8)
```

I want to get a feel for the data that I'll be working with, so I'll use pandas' ```read_csv()``` function to import the ```iris.data``` dataset and store its dataframe in a variable called ```iris```. I will then use the ```.head()``` method from the ```iris``` dataframe to get a snapshot of the dataset.

```python
# data URL: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
iris = pd.read_csv("iris.data", header=None)
iris.head()
```

... which gave me the following output:

|-|-|-|-|-|-|
|&nbsp;&nbsp;&nbsp;|0&nbsp;&nbsp;&nbsp;|1&nbsp;&nbsp;&nbsp;|2&nbsp;&nbsp;&nbsp;|3&nbsp;&nbsp;&nbsp;|4&nbsp;&nbsp;&nbsp;|
|0&nbsp;&nbsp;&nbsp;|5.1&nbsp;&nbsp;&nbsp;|3.5&nbsp;&nbsp;&nbsp;|1.4&nbsp;&nbsp;&nbsp;|0.2&nbsp;&nbsp;&nbsp;|Iris-setosa&nbsp;&nbsp;&nbsp;|
|1&nbsp;&nbsp;&nbsp;|4.9&nbsp;&nbsp;&nbsp;|3.0&nbsp;&nbsp;&nbsp;|1.4&nbsp;&nbsp;&nbsp;|0.2&nbsp;&nbsp;&nbsp;|Iris-setosa&nbsp;&nbsp;&nbsp;|
|2&nbsp;&nbsp;&nbsp;|4.7&nbsp;&nbsp;&nbsp;|3.2&nbsp;&nbsp;&nbsp;|1.3&nbsp;&nbsp;&nbsp;|0.2&nbsp;&nbsp;&nbsp;|Iris-setosa&nbsp;&nbsp;&nbsp;|
|3&nbsp;&nbsp;&nbsp;|4.6&nbsp;&nbsp;&nbsp;|3.1&nbsp;&nbsp;&nbsp;|1.5&nbsp;&nbsp;&nbsp;|0.2&nbsp;&nbsp;&nbsp;|Iris-setosa&nbsp;&nbsp;&nbsp;|
|4&nbsp;&nbsp;&nbsp;|5.0&nbsp;&nbsp;&nbsp;|3.6&nbsp;&nbsp;&nbsp;|1.4&nbsp;&nbsp;&nbsp;|0.2&nbsp;&nbsp;&nbsp;|Iris-setosa&nbsp;&nbsp;&nbsp;|

<br>

I (or, rather, my instructor) had me label the data based on a codebook and drop all the blank columns:

```python
iris.columns = ["sepal_length", "sepal_width", "petal_length", "petal_width", "species"]
iris.dropna(how='all', inplace=True)
iris.head()
```

... which gave me the following output:

|-|-|-|-|-|-|
|&nbsp;&nbsp;&nbsp;|septal_length&nbsp;&nbsp;&nbsp;|septal_width&nbsp;&nbsp;&nbsp;|petal_length&nbsp;&nbsp;&nbsp;|petal_width&nbsp;&nbsp;&nbsp;|species&nbsp;&nbsp;&nbsp;|
|0&nbsp;&nbsp;&nbsp;|5.1&nbsp;&nbsp;&nbsp;|3.5&nbsp;&nbsp;&nbsp;|1.4&nbsp;&nbsp;&nbsp;|0.2&nbsp;&nbsp;&nbsp;|Iris-setosa&nbsp;&nbsp;&nbsp;|
|1&nbsp;&nbsp;&nbsp;|4.9&nbsp;&nbsp;&nbsp;|3.0&nbsp;&nbsp;&nbsp;|1.4&nbsp;&nbsp;&nbsp;|0.2&nbsp;&nbsp;&nbsp;|Iris-setosa&nbsp;&nbsp;&nbsp;|
|2&nbsp;&nbsp;&nbsp;|4.7&nbsp;&nbsp;&nbsp;|3.2&nbsp;&nbsp;&nbsp;|1.3&nbsp;&nbsp;&nbsp;|0.2&nbsp;&nbsp;&nbsp;|Iris-setosa&nbsp;&nbsp;&nbsp;|
|3&nbsp;&nbsp;&nbsp;|4.6&nbsp;&nbsp;&nbsp;|3.1&nbsp;&nbsp;&nbsp;|1.5&nbsp;&nbsp;&nbsp;|0.2&nbsp;&nbsp;&nbsp;|Iris-setosa&nbsp;&nbsp;&nbsp;|
|4&nbsp;&nbsp;&nbsp;|5.0&nbsp;&nbsp;&nbsp;|3.6&nbsp;&nbsp;&nbsp;|1.4&nbsp;&nbsp;&nbsp;|0.2&nbsp;&nbsp;&nbsp;|Iris-setosa&nbsp;&nbsp;&nbsp;|

<br>

__Task 3: Getting a visual__

Alright, I have my dataset loaded. Now, I want to see what it would look like when plotting two metrics onto a chart. So, I write:

```python
sns.scatterplot(x = iris.sepal_length, y = iris.sepal_width, hue = iris.species, style = iris.species)
```

... and get the following chart as output:

![figure 1]({{site.baseurl}}/assets/posts/1/figure_1.png)

Nice! Let's get to the PCA part:

__Task 4: Standardising the data__

This is fairly simple. I will import the ```StandardScaler``` class from scikit-learn's preprocessing set of tools, and apply it to the feature space:

```python
X = iris.iloc[:, 0:4].values
y = iris.species.values

from sklearn.preprocessing import StandardScaler
X = StandardScaler().fit_transform(X)
```

... and I'm ready to move on to the next task.

__Task 5: Working out the eigenvectors and eigenvalues__

Covariance is the mean value of the product of the deviations of two variables from their respective means. A covariance matrix is a kind of matrix that holds the covariance of two variables given the row and column of each variable in the matrix. The formulae for both the covariance $$\sigma_{jk}$$ and covariance matrix $$\Sigma$$ are:

$$\sigma_{jk} = \frac{1}{n-1} \sum_{n=1}^{N} (x_{ij} - \bar{x}_j) (x_{ik} - \bar{x}_k)$$

and

$$\Sigma = \frac{1}{n-1} ((X - \bar{x})^T (X - \bar{x}))$$

Rather than reimplement the computational matrix algebra, I will use numpy's ```.cov()``` function and the matrix object's ```T``` object to express the iris matrix in terms of its transposition:

```python
covariance_matrix = np.cov(X.T)
print("Covariance matrix: \n", covariance_matrix)
```

... which game me an output of:

```
Covariance matrix: 
 [[ 1.00671141 -0.11010327  0.87760486  0.82344326]
 [-0.11010327  1.00671141 -0.42333835 -0.358937  ]
 [ 0.87760486 -0.42333835  1.00671141  0.96921855]
 [ 0.82344326 -0.358937    0.96921855  1.00671141]]
```

Now, I can compute the eigendecomposition---which consists of both the eigenvalues and eigenvectors--- of the covariance matrix with numpy's ```.linalg.eig()``` function. (the eigendecomposition of the covariance matrix is defined as $$\Sigma = W \wedge W^{-1}$$

```python
eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)
print("Eigenvectors: \n", eigenvectors, "\n")
print("Eigenvalues: \n", eigenvalues, "\n")
```

... which gave me an output of:

```
Eigenvectors: 
 [[ 0.52237162 -0.37231836 -0.72101681  0.26199559]
 [-0.26335492 -0.92555649  0.24203288 -0.12413481]
 [ 0.58125401 -0.02109478  0.14089226 -0.80115427]
 [ 0.56561105 -0.06541577  0.6338014   0.52354627]] 

Eigenvalues: 
 [2.93035378 0.92740362 0.14834223 0.02074601] 
```

Now, I am ready to move on to the technique of Singular Value Decomposition (SVD)

__Task 6: Singular Value Decomposition (SVD)__

Singular value decomposition (SVD) is a method in linear algebra that generalises the eigenvectors and eigenvalues of a matrix with factorisation. SVD literally involves factoring a matrix---you know, like how you factored trinomials back in high school (thanks S. J. Gould for that analogy). I'll run the procedure of SVD with Python's ```linalg.svd``` function and input the transposed matrix of the iris data ```X.T```, and I write:

```python
eigen_vec_svd, s, v = np.linalg.svd(X.T)
eigen_vec_svd
```

... which gave me an output of:

```
array([[-0.52237162, -0.37231836,  0.72101681,  0.26199559],
       [ 0.26335492, -0.92555649, -0.24203288, -0.12413481],
       [-0.58125401, -0.02109478, -0.14089226, -0.80115427],
       [-0.56561105, -0.06541577, -0.6338014 ,  0.52354627]])
```

__Task 7: Picking the Principal Components__

The goal of this project is to take a multi-variate dataset and reduce it to a single (or smaller set of) dimension(s). I will first calculate the variance explained by each principal component with the formula:

$$VE = \frac{C_i}{\Sigma C} \times 100 $$

in where $$VE$$ is the variance explained, $$C_i$$ is the individual principal component expressed as an eigenvalue and $$\Sigma C$$ is the sum of all principal components eigenvalues. Expressing this in terms of Python, I get:

```python
variance_explained = [(i/sum(eigenvalues))*100 for i in eigenvalues]
variance_explained
```

... with output:

```
[72.77045209380134, 23.03052326768065, 3.6838319576273775, 0.5151926808906323]
```

And calculating the cumulative variance explained:

```python
cumulative_vairance_explained = np.cumsum(variance_explained)
cumulative_vairance_explained
```

... with output:

```
array([ 72.77045209,  95.80097536,  99.48480732, 100.        ])
```

It would also be nice to visualise how the variance explained is going, so I'm going to plot it:

```python
sns.lineplot(x = [1, 2, 3, 4], y = cumulative_vairance_explained)
plt.xlabel("Number of components")
plt.ylabel("Cumulative explained variance")
plt.title("Explained variance v. Number of components")
plt.show()
```
... and a rather lovely ggplot-styled plot comes out:

![figure 2]({{site.baseurl}}/assets/posts/1/figure_2.png)

__Task 8: Projecting data onto lower-dimensional linear subspace__

I have decided on using the first two principal components (as opposed to all four) to create my construct. I will first compute the projection matrix, the perform the PCA by working out the projection matrix's dot product:

```python
projection_matrix = (eigenvectors.T[:][:][:2]).T
print("Projection matrix: \n", projection_matrix)
```

... which gave me an output of:

```
Projection matrix: 
 [[ 0.52237162 -0.37231836]
 [-0.26335492 -0.92555649]
 [ 0.58125401 -0.02109478]
 [ 0.56561105 -0.06541577]]
```

And finally:

```python
X_pca = X.dot(projection_matrix)
```

Now, let's take a look at how my statistical construct explains the iris dataset for the species _Iris-setosa_, _Iris-versicolor_ and _Iris-virginica_:

```python
for species in ('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'):
    sns.scatterplot(X_pca[y==species, 0], X_pca[y==species, 1])
```

![figure 3]({{site.baseurl}}/assets/posts/1/figure_3.png)

I managed to reduce four dimensions down to two. Noice :D
